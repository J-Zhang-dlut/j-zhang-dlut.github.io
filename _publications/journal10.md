---
title: "A deep non-negative matrix factorization model for big data representation learning"
collection: publications
category: manuscripts
permalink: /publication/journal10
excerpt: ''
date: 2021-07-20
venue: 'Frontiers in Neurorobotics'
paperurl: 'https://www.frontiersin.org/journals/neurorobotics/articles/10.3389/fnbot.2021.701194/full'
citation: 'Chen Z, Jin S, Liu R, et al. A deep non-negative matrix factorization model for big data representation learning[J]. Frontiers in Neurorobotics, 2021, 15: 701194.'
---

Nowadays, deep representations have been attracting much attention owing to the great performance in various tasks. However, the interpretability of deep representations poses a vast challenge on real-world applications. To alleviate the challenge, a deep matrix factorization method with non-negative constraints is proposed to learn deep part-based representations of interpretability for big data in this paper. Specifically, a deep architecture with a supervisor network suppressing noise in data and a student network learning deep representations of interpretability is designed, which is an end-to-end framework for pattern mining. Furthermore, to train the deep matrix factorization architecture, an interpretability loss is defined, including a symmetric loss, an apposition loss, and a non-negative constraint loss, which can ensure the knowledge transfer from the supervisor network to the student network, enhancing the robustness of deep representations. Finally, extensive experimental results on two benchmark datasets demonstrate the superiority of the deep matrix factorization method.
